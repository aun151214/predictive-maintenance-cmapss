{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23ee47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Processing FD001 ...\n",
      "âœ… FD001: X_train (17631, 30, 24), y_train (17631,)\n",
      "âœ… FD001: X_test (9928, 30, 24), y_test (9928,)\n",
      "\n",
      "ðŸ”¹ Processing FD002 ...\n",
      "âœ… FD002: X_train (45959, 30, 24), y_train (45959,)\n",
      "âœ… FD002: X_test (26159, 30, 24), y_test (26159,)\n",
      "\n",
      "ðŸ”¹ Processing FD003 ...\n",
      "âœ… FD003: X_train (21720, 30, 24), y_train (21720,)\n",
      "âœ… FD003: X_test (13379, 30, 24), y_test (13379,)\n",
      "\n",
      "ðŸ”¹ Processing FD004 ...\n",
      "âœ… FD004: X_train (53779, 30, 24), y_train (53779,)\n",
      "âœ… FD004: X_test (33593, 30, 24), y_test (33593,)\n"
     ]
    }
   ],
   "source": [
    "# 02_preprocessing.ipynb (multi-dataset)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# =========================\n",
    "# Parameters\n",
    "# =========================\n",
    "SEQ_LEN = 30\n",
    "MAX_RUL = 125\n",
    "DATA_DIR = \"../data/raw/\"\n",
    "PROCESSED_DIR = \"../data/processed/\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Column definitions\n",
    "# =========================\n",
    "sensor_cols = [f'sensor_{i}' for i in range(1, 22)]\n",
    "op_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3']\n",
    "cols = ['engine_id', 'cycle'] + op_cols + sensor_cols\n",
    "\n",
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "def gen_sequence(df, seq_length, features):\n",
    "    data_array = df[features].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "def gen_labels(df, seq_length, label):\n",
    "    label_array = df[label].values\n",
    "    return label_array[seq_length:]\n",
    "\n",
    "def preprocess_dataset(fd_id):\n",
    "    print(f\"\\nðŸ”¹ Processing {fd_id} ...\")\n",
    "\n",
    "    # File paths\n",
    "    train_file = os.path.join(DATA_DIR, f\"train_{fd_id}.txt\")\n",
    "    test_file = os.path.join(DATA_DIR, f\"test_{fd_id}.txt\")\n",
    "    rul_file = os.path.join(DATA_DIR, f\"RUL_{fd_id}.txt\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Training data\n",
    "    # -------------------------\n",
    "    train_df = pd.read_csv(train_file, sep=' ', header=None)\n",
    "    train_df = train_df.dropna(axis=1, how='all')\n",
    "    train_df.columns = cols\n",
    "\n",
    "    # Compute RUL for train\n",
    "    max_cycles = train_df.groupby('engine_id')['cycle'].max().reset_index()\n",
    "    max_cycles.columns = ['engine_id', 'max_cycle']\n",
    "    train_df = train_df.merge(max_cycles, on='engine_id')\n",
    "    train_df['RUL'] = train_df['max_cycle'] - train_df['cycle']\n",
    "    train_df['RUL'] = train_df['RUL'].clip(upper=MAX_RUL)\n",
    "\n",
    "    # Scale\n",
    "    scaler = MinMaxScaler()\n",
    "    train_df[op_cols + sensor_cols] = scaler.fit_transform(train_df[op_cols + sensor_cols])\n",
    "\n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, os.path.join(PROCESSED_DIR, f\"scaler_{fd_id}.pkl\"))\n",
    "\n",
    "    # Generate train sequences\n",
    "    feature_cols = op_cols + sensor_cols\n",
    "    sequence_list, label_list = [], []\n",
    "    for eid in train_df['engine_id'].unique():\n",
    "        engine_df = train_df[train_df['engine_id'] == eid]\n",
    "        seqs = list(gen_sequence(engine_df, SEQ_LEN, feature_cols))\n",
    "        labs = gen_labels(engine_df, SEQ_LEN, 'RUL')\n",
    "        sequence_list.extend(seqs)\n",
    "        label_list.extend(labs)\n",
    "\n",
    "    X_train = np.array(sequence_list)\n",
    "    y_train = np.array(label_list)\n",
    "\n",
    "    # Save arrays\n",
    "    np.save(os.path.join(PROCESSED_DIR, f\"X_train_{fd_id}.npy\"), X_train)\n",
    "    np.save(os.path.join(PROCESSED_DIR, f\"y_train_{fd_id}.npy\"), y_train)\n",
    "\n",
    "    print(f\"âœ… {fd_id}: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Test data\n",
    "    # -------------------------\n",
    "    test_df = pd.read_csv(test_file, sep=' ', header=None)\n",
    "    test_df = test_df.dropna(axis=1, how='all')\n",
    "    test_df.columns = cols\n",
    "\n",
    "    # True RUL values\n",
    "    rul_df = pd.read_csv(rul_file, sep=' ', header=None)\n",
    "    rul_df = rul_df.dropna(axis=1, how='all')\n",
    "    rul_df.columns = ['RUL']\n",
    "\n",
    "    # Compute test RUL\n",
    "    max_cycles = test_df.groupby('engine_id')['cycle'].max().reset_index()\n",
    "    max_cycles.columns = ['engine_id', 'max_cycle']\n",
    "    test_df = test_df.merge(max_cycles, on='engine_id')\n",
    "    test_df = test_df.merge(rul_df, left_on='engine_id', right_index=True)\n",
    "    test_df['RUL'] = test_df['RUL'] + test_df['max_cycle'] - test_df['cycle']\n",
    "    test_df['RUL'] = test_df['RUL'].clip(upper=MAX_RUL)\n",
    "\n",
    "    # Apply same scaler\n",
    "    test_df[op_cols + sensor_cols] = scaler.transform(test_df[op_cols + sensor_cols])\n",
    "\n",
    "    # Generate test sequences\n",
    "    sequence_list, label_list = [], []\n",
    "    for eid in test_df['engine_id'].unique():\n",
    "        engine_df = test_df[test_df['engine_id'] == eid]\n",
    "        seqs = list(gen_sequence(engine_df, SEQ_LEN, feature_cols))\n",
    "        labs = gen_labels(engine_df, SEQ_LEN, 'RUL')\n",
    "        sequence_list.extend(seqs)\n",
    "        label_list.extend(labs)\n",
    "\n",
    "    X_test = np.array(sequence_list)\n",
    "    y_test = np.array(label_list)\n",
    "\n",
    "    # Save arrays\n",
    "    np.save(os.path.join(PROCESSED_DIR, f\"X_test_{fd_id}.npy\"), X_test)\n",
    "    np.save(os.path.join(PROCESSED_DIR, f\"y_test_{fd_id}.npy\"), y_test)\n",
    "\n",
    "    print(f\"âœ… {fd_id}: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run for all FD sets\n",
    "# =========================\n",
    "for fd_id in [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]:\n",
    "    preprocess_dataset(fd_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
